{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    from hgtk.text import compose, decompose\n",
    "except:\n",
    "    !pip install hgtk\n",
    "    from hgtk.text import compose, decompose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 시 데이터 셋 :  백석 시 10편\n",
    "----\n",
    "\n",
    "이번 시간에는 \"RNN을 활용한 시 작문하기\"를 실습해보도록 하겠습니다.<br>\n",
    "시는 백석 시인의 시 10편을 선별하여 학습하고, <br>\n",
    "모델이 백석 시인의 작품을 생성할 수 있는지 확인해보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 백석 시의 편 수 :  10\n",
      "가난한 내가\n",
      "아름다운 나타샤를 사랑해서 \n",
      "오늘밤은 푹푹 눈이 나린다 \n",
      "나타샤를 사랑은 하고 \n",
      "눈은 푹푹 날리고 \n",
      "나는 혼자 쓸쓸히 앉어 소주를 마신다 \n",
      "소주를 마시며 생각한다 \n",
      "나타샤와 나는 눈이 푹푹 쌓이는 밤\n",
      "흰 당나귀 타고 산골로 가자 출출이 우는\n",
      "깊은 산골로 가 마가리에 살자 \n",
      "눈은 푹푹 나리고 \n",
      "나는 나타샤를 생각하고 \n",
      "나타샤가 아니올 리 없다 \n",
      "언제 벌써 내 속에 고조곤히 와 이야기한다 \n",
      "산골로 가는 것은 세상한테 지는 것이 아니다 \n",
      "세상 같은 건 더러워 버리는 것이다 \n",
      "눈은 푹푹 나리고 \n",
      "아름다운 나타샤는 나를 사랑하고 \n",
      "어데서 흰 당나귀도 오늘밤이 좋아서 응앙응앙 울을 것이다 \n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(\"./poets.zip\"):\n",
    "    !wget https://pai-datasets.s3.ap-northeast-2.amazonaws.com/alai-deeplearning/poets.zip\n",
    "if not os.path.exists(\"./poets\"):\n",
    "    !unzip poets.zip\n",
    "    \n",
    "def read_poets(poet_dir):\n",
    "    \"\"\"시가 담겨져 있는 디렉토리 내에서 시 내용들을 가져오는 함수\n",
    "    :param poet_dir : *.txt 포맷으로 저장된 시가 담긴 디렉토리 위치\n",
    "    \n",
    "    :return \n",
    "        list of the poet(type: str)\n",
    "    \"\"\"\n",
    "    poets = []\n",
    "    for poet_name in os.listdir(poet_dir):\n",
    "        poet_path = os.path.join(poet_dir, poet_name)\n",
    "        with open(poet_path,'r') as f:\n",
    "            poets.append(f.read())\n",
    "    return poets\n",
    "\n",
    "# 백석 시를 읽어옮\n",
    "poets = read_poets(\"poets/\")\n",
    "print(\"총 백석 시의 편 수 : \",len(poets))\n",
    "\n",
    "print(poets[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# \\[ 시를 작문하는 RNN 모델 구성하기 \\]\n",
    "----\n",
    "----\n",
    "\n",
    "> *백석 시를 학습하고, 시 문구를 넣었을 때, 백석 시와 비슷한 시를 생성해내는 모델을 만들어 보도록 하겠습니다.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 1. Word Embedding 하기\n",
    "---\n",
    "---\n",
    "\n",
    "* 우리는 한글 자모자를 기준으로 임베딩을 하도록 하겠습니다.<Br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) 한글 자모자 구성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한글 자모자를 인덱스로 만드는 Map 구현\n",
    "초성 = (\n",
    "    u'ㄱ', u'ㄲ', u'ㄴ', u'ㄷ', u'ㄸ', u'ㄹ', u'ㅁ', u'ㅂ', u'ㅃ', u'ㅅ',\n",
    "    u'ㅆ', u'ㅇ', u'ㅈ', u'ㅉ', u'ㅊ', u'ㅋ', u'ㅌ', u'ㅍ', u'ㅎ'\n",
    ")\n",
    "\n",
    "중성 = (\n",
    "    u'ㅏ', u'ㅐ', u'ㅑ', u'ㅒ', u'ㅓ', u'ㅔ', u'ㅕ', u'ㅖ', u'ㅗ', u'ㅘ',\n",
    "    u'ㅙ', u'ㅚ', u'ㅛ', u'ㅜ', u'ㅝ', u'ㅞ', u'ㅟ', u'ㅠ', u'ㅡ', u'ㅢ', u'ㅣ'\n",
    ")\n",
    "\n",
    "종성 = (\n",
    "    u'', u'ㄱ', u'ㄲ', u'ㄳ', u'ㄴ', u'ㄵ', u'ㄶ', u'ㄷ', u'ㄹ', u'ㄺ',\n",
    "    u'ㄻ', u'ㄼ', u'ㄽ', u'ㄾ', u'ㄿ', u'ㅀ', u'ㅁ', u'ㅂ', u'ㅄ', u'ㅅ',\n",
    "    u'ㅆ', u'ㅇ', u'ㅈ', u'ㅊ', u'ㅋ', u'ㅌ', u'ㅍ', u'ㅎ'\n",
    ")\n",
    "\n",
    "미포함종성 = tuple(set(종성) - set(초성)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) 임베딩 dictionary 구현하기\n",
    "\n",
    "\n",
    "| 자모자 | 인덱스 |\n",
    "| ---- | --- |\n",
    "|ㄱ| 0 |\n",
    "|ㄲ| 1 |\n",
    "|ㄴ| 2 |\n",
    "|ㄷ| 3 |\n",
    "|ㄸ| 4 |\n",
    "|ㄹ| 5 |\n",
    "|...| ... |\n",
    "\n",
    "와 같은 순으로 매칭시키도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 초성, 중성, 종성, 그리고 \"ᴥ\"를 포함\n",
    "jamos = list(초성 + 중성 + 미포함종성 + (\"ᴥ\",\" \",\"\\n\",)) \n",
    "# jamo에 매칭되는 인덱스\n",
    "jamo2idx = { jamo : idx for idx, jamo in enumerate(jamos) }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 2. Language Modeling\n",
    "---\n",
    "---\n",
    "\n",
    "* **언어 모델(language model)** 은 문장, 즉 단어 나열에 확률을 부여하고, 예측하는 모델을 말합니다. 특정한 단어의 시퀀스에 대해, 그렇게 단어가 배치될 가능성이 어느 정도인지(얼마나 자연스러운 단어 순서인지)를 확률로 평가하는 것입니다.\n",
    "* 예를 들어, \"나 밥 먹으러 가\"라는 단어 시퀀스에는 높은 확률(예:0.092)를 출력하고, \"나 비행기 먹으러 가\"라는 시퀀스에는 낮은 확률(예:0.00001)을 출력하는 것이 일종의 언어 모델입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) 문장 확률(동시확률) 정의\n",
    "\n",
    "하나의 단어(word)를 w라고 합시다. 단어의 시퀀스인 전체 문장(sentence)를 대문자 W라고 합시다. n개의 w로 구성된 문장 W의 확률은 다음과 같이 표현할 수 있습니다.\n",
    "\n",
    "$\n",
    "P(W) = p(w_1,w_2,w_3,\\cdots,w_n) = \\prod_{i=1}^n p(w_i)\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) 조건부 언어 모델(conditional language model)\n",
    "\n",
    "우리는 이미 앞서 등장한 단어를 바탕으로 다음 단어를 예측할 수 있습니다. 예를 들어, n-1개의 단어가 나열된 상태에서 n번째 단어의 확률을 다음과 같이 표현할 수 있습니다.<br>\n",
    "$\n",
    "P(w_n|w_1,w_2,w_3,\\cdots,w_{n-1})\n",
    "$\n",
    "\n",
    "예를 들어 다섯번째 단어의 확률은 아래처럼 표기할 수 있습니다.<br>\n",
    "\n",
    "$\n",
    "P(w_5|w_1,w_2,w_3,w_4)\n",
    "$\n",
    "\n",
    "위에서 구한 문장의 확률은 위의 조건부 확률로 분해해서 바라볼 수 있습니다.<br>\n",
    "\n",
    "$\n",
    "P(w_1,\\cdots,w_n) = P(w_n|w_1,\\cdots,w_{n-1})P(w_{n-1}|w_1,\\cdots,w_{n-2})\\\\\n",
    "\\cdots P(w_3|w_1,w_2)P(w_2|w_1)P(w_1) \\\\\n",
    "= \\prod_{i=1}^{n}p(w_t|w_1,\\cdots,w_{i=1})\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) 언어 모델의 직관적 해석\n",
    "\n",
    "````\n",
    "KTX를 타러 서울역에 도착했는데, 짐 싸다 늦어서 KTX를 [?]\n",
    "````\n",
    "\n",
    "라는 문장이 있다고 생각해봅시다. 우리는 앞서 나열된 단어들을 통해 직관적으로 `[?]`에 들어갈 내용은 \"놓쳤다\"임을 예상할 수 있습니다. 우리는 사전의 나열된 정보를 바탕으로 사후적으로 다음 단어를 판단할 수 있기 때문입니다. <br>\n",
    "언어 모델도 동일하게 단어의 시퀀스를 통해, 다음 단어가 무엇인지 나올지를 판단하도록 학습함으로써, 문장의 맥락(Context)를 파악하는 방법을 학습하게 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 3. Poet Generator 구현하기\n",
    "---\n",
    "---\n",
    "\n",
    "* 우리는 이전 시간에 다룬 삼성 Stock을 예측하는 모델과 같이 Stacked LSTM을 이용해 보도록 하겠습니다.\n",
    "* 30개의 자모자를 읽고, 다음 자모자를 예측하는 Language Model을 학습시키도록 하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) 단어를 벡터로 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_steps = 30 # 몇개의 자모자를 보고 다음 자모자를 예측할지 결정\n",
    "word_ndims = len(jamos) # 임베딩한 자모자의 갯수\n",
    "\n",
    "data = []\n",
    "targets = []\n",
    "for poet in poets:\n",
    "    jamo_poet = decompose(poet) # 시의 음절을 자모자로 분리\n",
    "     # 벡터화한 후 stack\n",
    "    jamo_vectors = np.stack([jamo2idx[char] \n",
    "                             for char in jamo_poet \n",
    "                             if char in jamo2idx])\n",
    "\n",
    "    len_seqs = len(jamo_vectors)\n",
    "    for i in range(len_seqs-time_steps):\n",
    "        # 하나씩 지나가면서, 데이터를 확보\n",
    "        datum = jamo_vectors[i:i+time_steps]\n",
    "        target = jamo_vectors[i+1:i+time_steps+1]\n",
    "        data.append(datum)\n",
    "        targets.append(target)\n",
    "        \n",
    "X = np.array(data)\n",
    "Y = np.array(targets)\n",
    "\n",
    "merged = list(zip(X,Y))\n",
    "\n",
    "np.random.shuffle(merged)\n",
    "\n",
    "X, Y = list(zip(*merged))\n",
    "X = np.stack(X)\n",
    "Y = np.stack(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14704, 30)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape # 총 14704의 문장, 30개로 이루어진 time step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14704, 30)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape # 우리가 예측하려는 문장도 같다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 4. RNN 모델 만들기\n",
    "---\n",
    "---\n",
    "\n",
    "* 우리는 이전 시간에 다룬 삼성 Stock을 예측하는 모델과 같이 Stacked LSTM을 이용해 보도록 하겠습니다.\n",
    "* 30개의 자모자를 읽고, 다음 자모자를 예측하는 Language Model을 학습시키도록 하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Layer 구성하기\n",
    "\n",
    "이번 경우, Inference 때와 Training 때 약간 다르게 동작합니다.<br>\n",
    "이를 위해, 우리는 `keras.layers`에서의 Layer을 우선 선언한 후, 두 모델을 만들어야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import LSTM, Input, Dense, Embedding\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "\n",
    "_, n_steps = X.shape\n",
    "n_inputs = np.max(X) + 1\n",
    "\n",
    "n_embedding = 16\n",
    "n_units = 200\n",
    "\n",
    "# 우선 Keras Layer를 구성\n",
    "embedding_layer = Embedding(n_inputs, n_embedding) # 총 55개의 자모자(n_inputs)를 16차원(n_embedding)으로 줄입니다.\n",
    "lstm1_layer = LSTM(units=n_units, \n",
    "                   return_sequences=True, \n",
    "                   return_state=True)\n",
    "lstm2_layer = LSTM(units=n_units, \n",
    "                   return_sequences=True,\n",
    "                   return_state=True)\n",
    "lstm3_layer = LSTM(units=n_units,\n",
    "                   return_sequences=True,\n",
    "                   return_state=True)\n",
    "dense_layer = Dense(n_inputs, activation='softmax')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Training Model 구성하기\n",
    "\n",
    "아래와 같이 구성할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"training\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inputs (InputLayer)          [(None, None)]            0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, None, 16)          880       \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  [(None, None, 200), (None 173600    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                [(None, None, 200), (None 320800    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                [(None, None, 200), (None 320800    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, None, 55)          11055     \n",
      "=================================================================\n",
      "Total params: 827,135\n",
      "Trainable params: 827,135\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = Input(shape=(None,), name='inputs')\n",
    "\n",
    "embeded = embedding_layer(inputs)\n",
    "hidden1, _, _ = lstm1_layer(embeded)\n",
    "hidden2, _, _ = lstm2_layer(hidden1)\n",
    "hidden3, _, _ = lstm3_layer(hidden2)\n",
    "output = dense_layer(hidden3)\n",
    "\n",
    "model = Model(inputs, output, name='training')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Inference Model 구성하기\n",
    "\n",
    "Training Model과 Inference Model은 같은 Weight를 공유하고 있는데, 차이는 Inference Model에서는 state 정보를 주입받아, 계산된 state를 반환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"inference\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "inputs (InputLayer)             [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, None, 16)     880         inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "h1_input (InputLayer)           [(None, 200)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "c1_input (InputLayer)           [(None, 200)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, None, 200),  173600      embedding_2[0][0]                \n",
      "                                                                 h1_input[0][0]                   \n",
      "                                                                 c1_input[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "h2_input (InputLayer)           [(None, 200)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "c2_input (InputLayer)           [(None, 200)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 200),  320800      lstm[5][0]                       \n",
      "                                                                 h2_input[0][0]                   \n",
      "                                                                 c2_input[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "h3_input (InputLayer)           [(None, 200)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "c3_input (InputLayer)           [(None, 200)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, None, 200),  320800      lstm_1[5][0]                     \n",
      "                                                                 h3_input[0][0]                   \n",
      "                                                                 c3_input[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 55)     11055       lstm_2[5][0]                     \n",
      "==================================================================================================\n",
      "Total params: 827,135\n",
      "Trainable params: 827,135\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Inference Model을 구성\n",
    "\n",
    "## Initial State을 주입할 수 있도록 구성\n",
    "h1_in = Input(shape=(n_units,), name='h1_input')\n",
    "c1_in = Input(shape=(n_units,), name='c1_input')\n",
    "h2_in = Input(shape=(n_units,), name='h2_input')\n",
    "c2_in = Input(shape=(n_units,), name='c2_input')\n",
    "h3_in = Input(shape=(n_units,), name='h3_input')\n",
    "c3_in = Input(shape=(n_units,), name='c3_input')\n",
    "\n",
    "in_states_1 = [h1_in, c1_in]\n",
    "in_states_2 = [h2_in, c2_in]\n",
    "in_states_3 = [h3_in, c3_in]\n",
    "in_states = in_states_1 + in_states_2 + in_states_3\n",
    "\n",
    "# 모델 구성하기\n",
    "embedding_layer = Embedding(n_inputs, n_embedding)\n",
    "hidden1, h1_out, c1_out = lstm1_layer(embeded,\n",
    "                                      initial_state=in_states_1)\n",
    "hidden2, h2_out, c2_out = lstm2_layer(hidden1,\n",
    "                                      initial_state=in_states_2)\n",
    "hidden3, h3_out, c3_out = lstm3_layer(hidden2,\n",
    "                                      initial_state=in_states_3)\n",
    "output = dense_layer(hidden3)\n",
    "\n",
    "## output state를 받을 수 있도록 구성\n",
    "out_states_1 = [h1_out, c1_out]\n",
    "out_states_2 = [h2_out, c2_out]\n",
    "out_states_3 = [h3_out, c3_out]\n",
    "out_states = out_states_1 + out_states_2 + out_states_3\n",
    "\n",
    "inference_model = Model([inputs]+in_states, \n",
    "                        [output]+out_states_1+out_states_2+out_states_3,\n",
    "                        name='inference')\n",
    "inference_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) Poet Generate 메소드 구현하기\n",
    "\n",
    "우리는 첫 소절을 입력받아, 다음 글자를 생성하는 메소드를 구현하도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_poet(inference_model, test_string, generate_length=200):\n",
    "    \"\"\" 주어진 첫 소절을 바탕으로, 이어서 쓰는 시\n",
    "    \"\"\"\n",
    "    jamo_seqs = decompose(test_input_string)\n",
    "    code_seqs = np.stack([jamo2idx[char] for char in jamo_seqs])\n",
    "    initial_state = [np.zeros((1,200)) for _ in range(6)]\n",
    "\n",
    "    outputs = inference_model.predict(\n",
    "        [code_seqs[np.newaxis]]+initial_state)\n",
    "    logits, states = outputs[0], outputs[1:]\n",
    "    last_logits = logits[:,-1:,:] # 마지막 출력값은 다음 입력값이 됨\n",
    "    last_word = np.argmax(last_logits, axis=-1)\n",
    "    \n",
    "    generated_sequence = [last_word]\n",
    "    for _ in range(generate_length):\n",
    "        # 입력값 + states 값을 Input으로 넣어줌\n",
    "        outputs = inference_model.predict(\n",
    "            [last_word] + states)\n",
    "        logits, states = outputs[0], outputs[1:]\n",
    "        \n",
    "        # 마지막 출력값은 다음 입력값이 됨\n",
    "        last_logits = logits[:,-1:,:]\n",
    "        last_word = np.argmax(last_logits, axis=-1)\n",
    "        generated_sequence.append(last_word)\n",
    "\n",
    "    generated_sequence = np.stack(generated_sequence).ravel()    \n",
    "    generated_poet = compose(\"\".join([jamos[code]\n",
    "                                      for code in generated_sequence]))\n",
    "    return generated_poet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test input string >>> 내 어머니와 아버지는\n",
      "ㅒㅒㅒㅒㅒㅒㅒㅎㅎㅎㅎㅎㅎㅎㅎㅎㅍㅍㅍㅍㅍ퍠ㅒㅒㅒㄷㄷㄷㄷㄷㄷㄷ됴ㅛㅆㅆㅆㅆ쏘ㅗㅗㅗㅗㅋㅋㅅㅅㅅㅅㅅㅅㅅㅅㅌㅌㅌㅌㅌㅌㅌㅌㅌㅌㅌㅌㅅㅅㅅㅅㅅㅌㅌㅌㅌㅌㅌㅌㅌㅌㅌㅅㅅㅅㅅㅅㅌㅌㅌㅌㅌㅌㅌㅌㅌㅌㅅㅅㅅㅅㅅㅌㅌㅌㅌㅌㅌㅌㅌㅌㅌㅅㅅㅅㅅㅅㅌㅌㅌㅌㅌㅌㅌㅌㅌㅌㅅㅅㅅㅅㅅㅌㅌㅌㅌㅌㅌㅌㅌㅌㅌㅅㅅㅅㅅㅅㅌㅌㅌㅌㅌㅌㅌㅌㅌㅌㅅㅅㅅㅅㅅㅌㅌㅌㅌㅌㅌㅌㅌㅌㅌㅅㅅㅅㅅㅅㅌㅌㅌㅌㅌㅌㅌㅌㅌㅌㅅㅅㅅㅅㅅㅌㅌㅌㅌ\n"
     ]
    }
   ],
   "source": [
    "test_input_string = \"내 어머니와 아버지는\"\n",
    "print(f\"test input string >>> {test_input_string}\")\n",
    "print(generate_poet(inference_model, test_input_string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습이 아직 되지 않았기 때문에, 이상하게 나타납니다. 학습함에 따라 점점 글자를 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) 모델 학습시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14704/14704 [==============================] - 117s 8ms/sample - loss: 0.9663\n",
      "test : 흰 당나귀 타고 산골로 가자 출출이 우는 >\n",
      "\n",
      "\n",
      "것이다\n",
      "이것은 오는 것이다\n",
      "이것은 오는 것이다\n",
      "이것은 오는 것이다\n",
      "이것은 오는 것이다\n",
      "이것은 오는 것이다\n",
      "이것은 오는 것이다\n",
      "이\n",
      "14704/14704 [==============================] - 121s 8ms/sample - loss: 0.7244\n",
      "test : 흰 당나귀 타고 산골로 가자 출출이 우는 >\n",
      "\n",
      "\n",
      "것이었다\n",
      "그러나 줌시 다하와 하긋한 아븜우로 가는 것이 있다\n",
      "내 가난한 늙은 어머니가\n",
      "이렇게 서녁 간 당다는 만 옛적 큰 아바지가 \n",
      "14704/14704 [==============================] - 116s 8ms/sample - loss: 0.5626\n",
      "test : 흰 당나귀 타고 산골로 가자 출출이 우는 >\n",
      "\n",
      "\n",
      "것이었다\n",
      "내 가슴이 없는 것이다\n",
      "이것은 아득한 옛날 한가하고 조이틀은 아이들끼리 앗간 한 방을 잡고 조아질하고 쌈방이 굴리고 \n",
      "14704/14704 [==============================] - 118s 8ms/sample - loss: 0.4753\n",
      "test : 흰 당나귀 타고 산골로 가자 출출이 우는 >\n",
      "\n",
      "\n",
      "깊은 산골로 가자 출출이 우는\n",
      "깊은 산골로 가자 출출이 우는\n",
      "깊은 산골로 가자 출출이 우는\n",
      "깊은 산골로 가자 출출이 우는\n",
      "깊은\n",
      " 1440/14704 [=>............................] - ETA: 1:54 - loss: 0.4345"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-138-737821940bb3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# 평가\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    778\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m           \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m           steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3292\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 출력 라벨이 자모자의 인덱스로 나오기 때문에 우리는 sparse_categorical_crossentropy\n",
    "# 를 이용해야 합니다.\n",
    "model.compile(Adam(lr=1e-3),\n",
    "              loss=sparse_categorical_crossentropy)\n",
    "\n",
    "epochs=10\n",
    "test_input_string = \"내 어머니와 아버지는\"\n",
    "\n",
    "for _ in range(epochs):\n",
    "    model.fit(x=X,y=Y, batch_size=20)\n",
    "    \n",
    "    # 평가\n",
    "    print(\"test : {} >\".format(test_input_string))\n",
    "    print(generate_poet(inference_model, test_input_string))\n",
    "    \n",
    "    # 데이터 셋의 순서를 섞어줌\n",
    "    dataset = list(zip(X, Y))\n",
    "    np.random.shuffle(dataset)\n",
    "    X, Y = list(zip(*dataset))\n",
    "    X,Y = np.stack(X), np.stack(Y)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reference : \n",
    "1. [language model](https://wikidocs.net/21668)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
